{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "pyrmt.ipynb:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  \"  O. Ledoit and M. Wolf\\n\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pylab as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import sys\n",
    "import re\n",
    "import os.path\n",
    "import yfinance as yf \n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats.mstats import winsorize\n",
    "import os\n",
    "import glob\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "\n",
    "import scipy\n",
    "%run pyrmt.ipynb #other functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Ledoit & Wolf constant correlation unequal variance shrinkage estimator.\"\"\"\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "def shrinkage(returns: np.array) -> Tuple[np.array, float, float]:\n",
    "    \"\"\"Shrinks sample covariance matrix towards constant correlation unequal variance matrix.\n",
    "    Ledoit & Wolf (\"Honey, I shrunk the sample covariance matrix\", Portfolio Management, 30(2004),\n",
    "    110-119) optimal asymptotic shrinkage between 0 (sample covariance matrix) and 1 (constant\n",
    "    sample average correlation unequal sample variance matrix).\n",
    "    Paper:\n",
    "    http://www.ledoit.net/honey.pdf\n",
    "    Matlab code:\n",
    "    https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip\n",
    "    Special thanks to Evgeny Pogrebnyak https://github.com/epogrebnyak\n",
    "    :param returns:\n",
    "        t, n - returns of t observations of n shares.\n",
    "    :return:\n",
    "        Covariance matrix, sample average correlation, shrinkage.\n",
    "    \"\"\"\n",
    "    t, n = returns.shape\n",
    "    mean_returns = np.mean(returns, axis=0, keepdims=True)\n",
    "    returns -= mean_returns\n",
    "    sample_cov = returns.transpose() @ returns / t\n",
    "\n",
    "    # sample average correlation\n",
    "    var = np.diag(sample_cov).reshape(-1, 1)\n",
    "    sqrt_var = var ** 0.5\n",
    "    unit_cor_var = sqrt_var * sqrt_var.transpose()\n",
    "    average_cor = ((sample_cov / unit_cor_var).sum() - n) / n / (n - 1)\n",
    "    prior = average_cor * unit_cor_var\n",
    "    np.fill_diagonal(prior, var)\n",
    "\n",
    "    # pi-hat\n",
    "    y = returns ** 2\n",
    "    phi_mat = (y.transpose() @ y) / t - sample_cov ** 2\n",
    "    phi = phi_mat.sum()\n",
    "\n",
    "    # rho-hat\n",
    "    theta_mat = ((returns ** 3).transpose() @ returns) / t - var * sample_cov\n",
    "    np.fill_diagonal(theta_mat, 0)\n",
    "    rho = (\n",
    "        np.diag(phi_mat).sum()\n",
    "        + average_cor * (1 / sqrt_var @ sqrt_var.transpose() * theta_mat).sum()\n",
    "    )\n",
    "\n",
    "    # gamma-hat\n",
    "    gamma = np.linalg.norm(sample_cov - prior, \"fro\") ** 2\n",
    "\n",
    "    # shrinkage constant\n",
    "    kappa = (phi - rho) / gamma\n",
    "    shrink = max(0, min(1, kappa / t))\n",
    "\n",
    "    # estimator\n",
    "    sigma = shrink * prior + (1 - shrink) * sample_cov\n",
    "\n",
    "    return sigma, average_cor, shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrinkage_EMW(returns_tmp: np.array, lookback = 126) -> Tuple[np.array, float, float]:\n",
    "    \"\"\"Shrinks sample covariance matrix towards constant correlation unequal variance matrix.\n",
    "    Ledoit & Wolf (\"Honey, I shrunk the sample covariance matrix\", Portfolio Management, 30(2004),\n",
    "    110-119) optimal asymptotic shrinkage between 0 (sample covariance matrix) and 1 (constant\n",
    "    sample average correlation unequal sample variance matrix).\n",
    "    Paper:\n",
    "    http://www.ledoit.net/honey.pdf\n",
    "    Matlab code:\n",
    "    https://www.econ.uzh.ch/dam/jcr:ffffffff-935a-b0d6-ffff-ffffde5e2d4e/covCor.m.zip\n",
    "    Special thanks to Evgeny Pogrebnyak https://github.com/epogrebnyak\n",
    "    :param returns:\n",
    "        t, n - returns of t observations of n shares.\n",
    "    :return:\n",
    "        Covariance matrix, sample average correlation, shrinkage.\n",
    "    \"\"\"\n",
    "    returns = returns_tmp.tail(lookback).values\n",
    "    t, n = returns.shape\n",
    "    mean_returns = np.mean(returns, axis=0, keepdims=True) # make EWMA\n",
    "    returns -= mean_returns\n",
    "    COV_tmp = returns_tmp.ewm(span = lookback).cov()\n",
    "    idx = returns_tmp.index.get_level_values(0)[-1]\n",
    "    sample_cov = COV_tmp[COV_tmp.index.get_level_values(0) == idx]\n",
    "    sample_cov = sample_cov.values\n",
    "    #sample_cov = returns.transpose() @ returns / t\n",
    "\n",
    "    # sample average correlation\n",
    "    var = np.diag(sample_cov).reshape(-1, 1)\n",
    "    sqrt_var = var ** 0.5\n",
    "    unit_cor_var = sqrt_var * sqrt_var.transpose()\n",
    "    average_cor = ((sample_cov / unit_cor_var).sum() - n) / n / (n - 1)\n",
    "    prior = average_cor * unit_cor_var\n",
    "    np.fill_diagonal(prior, var)\n",
    "\n",
    "    # pi-hat\n",
    "    y = returns ** 2\n",
    "    phi_mat = (y.transpose() @ y) / t - sample_cov ** 2\n",
    "    phi = phi_mat.sum()\n",
    "\n",
    "    # rho-hat\n",
    "    theta_mat = ((returns ** 3).transpose() @ returns) / t - var * sample_cov\n",
    "    np.fill_diagonal(theta_mat, 0)\n",
    "    rho = (\n",
    "        np.diag(phi_mat).sum()\n",
    "        + average_cor * (1 / sqrt_var @ sqrt_var.transpose() * theta_mat).sum()\n",
    "    )\n",
    "\n",
    "    # gamma-hat\n",
    "    gamma = np.linalg.norm(sample_cov - prior, \"fro\") ** 2\n",
    "\n",
    "    # shrinkage constant\n",
    "    kappa = (phi - rho) / gamma\n",
    "    shrink = max(0, min(1, kappa / t))\n",
    "\n",
    "    # estimator\n",
    "    sigma = shrink * prior + (1 - shrink) * sample_cov\n",
    "\n",
    "    return sigma, average_cor, shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import ezodf\n",
    "import scipy.optimize as sco\n",
    "import scipy\n",
    "\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "def Optimize_Portfolio(data ,lookback = 126, risk_free = 0, objective = 'Kelly'):\n",
    "\n",
    "    ret = (data-1).mean()\n",
    "    #cov_fit = LedoitWolf().fit(data)\n",
    "    #cov = cov_fit.covariance_\n",
    "    cov, average_cor, shrink = shrinkage_EMW(data, lookback = lookback)\n",
    "    #cov = PCA_cov(data, N=5)\n",
    "   \n",
    "  \n",
    "    if objective == 'Max Div':\n",
    "        num_assets = len(data.columns)\n",
    "        args = (cov)\n",
    "        constraints = ({'type':'ineq', 'fun': lambda x: x},#all elements greater than one\n",
    "                  #{'type':'ineq', 'fun': lambda x: 1 - np.sum(x)} # sum <= 1\n",
    "                  {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}) \n",
    "        \n",
    "        result = sco.minimize(calc_diversification_ratio, num_assets*[1./num_assets,], args=args, \n",
    "                              method='SLSQP', constraints=constraints, tol = 0.0000000000000000000000001)\n",
    "        \n",
    "    elif objective == \"min var\":\n",
    "        num_assets = len(data.columns)\n",
    "        args = (cov)\n",
    "        constraints = ({'type':'ineq', 'fun': lambda x: x},#all elements greater than one\n",
    "                  #{'type':'ineq', 'fun': lambda x: 1 - np.sum(x)} # sum <= 1\n",
    "                  {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}) \n",
    "        \n",
    "        result = sco.minimize(port_var, num_assets*[1./num_assets,], args=args, \n",
    "                              method='SLSQP', constraints=constraints, tol = 0.0000000000000000000000001)\n",
    "    elif objective == \"erc\":\n",
    "        num_assets = len(data.columns) \n",
    "        args = (cov)\n",
    "        constraints = ({'type':'ineq', 'fun': lambda x: x},#all elements greater than one\n",
    "                  #{'type':'ineq', 'fun': lambda x: 1 - np.sum(x)} # sum <= 1\n",
    "                  {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},\n",
    "                      {'type':'ineq', 'fun': lambda x: x-(1/num_assets)*0.7}, # min position\n",
    "                      {'type':'ineq', 'fun': lambda x: (1/num_assets)*1.3-x}) # max position\n",
    "        \n",
    "        result = sco.minimize(erc, num_assets*[1./num_assets,], args=args, \n",
    "                              method='SLSQP', constraints=constraints, tol = 0.0000000000000000000000001)\n",
    "        \n",
    "\n",
    "    return (result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def port_var(weights, cov):\n",
    "    var = weights.dot(cov).dot(weights)\n",
    "    return(var)\n",
    "\n",
    "def port_ret(weights, ret, risk_free = 0):\n",
    "    #needs to be array\n",
    "    ret = ret - risk_free\n",
    "    port_ret = weights.dot(ret)\n",
    "    return(port_ret)\n",
    "\n",
    "def risk_parity(data):\n",
    "    vol = np.log((data)).std()\n",
    "\n",
    "    sum_vol = 0\n",
    "    for i in range(len(vol)):\n",
    "        sum_vol =sum_vol + (1/vol[i])\n",
    "    \n",
    "    weight = []\n",
    "    for i in range(len(vol)):\n",
    "        w = (1/vol[i])/(sum_vol)\n",
    "        weight.append(w)\n",
    "   \n",
    "    weight = [round(num, 2) for num in weight]\n",
    "    return(weight)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_diversification_ratio(weights, cov):\n",
    "    # average weighted vol\n",
    "    w_vol = np.dot(np.sqrt(np.diag(cov)), weights.T)\n",
    "    # portfolio vol\n",
    "    port_vol = np.sqrt(port_var(weights, cov))\n",
    "    \n",
    "    diversification_ratio = w_vol/port_vol\n",
    "    # return negative for minimization problem (maximize = minimize -)\n",
    "    return -diversification_ratio\n",
    "\n",
    "def erc(weights, cov):\n",
    "        # these are non normalized risk contributions, i.e. not regularized\n",
    "        # by total risk, seems to help numerically\n",
    "        risk_contributions = np.dot(weights, cov) * weights\n",
    "        a = np.reshape(risk_contributions, (len(risk_contributions), 1))\n",
    "        # broadcasts so you get pairwise differences in risk contributions\n",
    "        risk_diffs = a - a.transpose()\n",
    "        sum_risk_diffs_squared = np.sum(np.square(np.ravel(risk_diffs)))\n",
    "        # https://stackoverflow.com/a/36685019/1451311\n",
    "        return sum_risk_diffs_squared #/ scale_factorcov\n",
    "    \n",
    "\n",
    "\n",
    "import sklearn.datasets, sklearn.decomposition\n",
    "\n",
    "def PCA_cov(data, N = 5):\n",
    "    \n",
    "    X = data.ewm(span = 252).cov()\n",
    "    DATE_IDX = X.index.get_level_values(level=0)[-1]\n",
    "    X = X[X.index.get_level_values(0)==DATE_IDX].droplevel(0)\n",
    "    mu = np.mean(X, axis=0)\n",
    "\n",
    "    pca = sklearn.decomposition.PCA()\n",
    "    pca.fit(X)\n",
    "\n",
    "    nComp = N\n",
    "    Xhat = np.dot(pca.transform(X)[:,:nComp], pca.components_[:nComp,:])\n",
    "    Xhat += mu\n",
    "    clean_cov = pd.DataFrame(Xhat)\n",
    "    clean_cov.index = X.index\n",
    "    clean_cov.columns = X.index\n",
    "    return(clean_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERC_gestalt(data, lookback = 126):\n",
    "    \n",
    "    prices_df = pd.DataFrame()\n",
    "    for tick in data['Yahoo']:\n",
    "    \n",
    "        price = yf.download(tick,start='2000-01-01', progress = False, threads = False)\n",
    "        price = price['Adj Close']\n",
    "        prices_df[tick] = price\n",
    "    \n",
    "    log_ret = np.log(prices_df) - np.log(prices_df.shift(1))\n",
    "    log_ret = log_ret.dropna()\n",
    "    weight = Optimize_Portfolio(log_ret, lookback = lookback, objective='erc')['x'].round(3)\n",
    "\n",
    "    return(weight)\n",
    "\n",
    "def round_to_multiple(number, multiple):\n",
    "    return multiple * round(number / multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import inv,pinv\n",
    "from scipy.optimize import minimize\n",
    "\n",
    " # risk budgeting optimization\n",
    "def calculate_portfolio_var(w,V):\n",
    "    # function that calculates portfolio risk\n",
    "    w = np.matrix(w)\n",
    "    return (w*V*w.T)[0,0]\n",
    "\n",
    "def calculate_risk_contribution(w,V):\n",
    "    # function that calculates asset contribution to total risk\n",
    "    w = np.matrix(w)\n",
    "    sigma = np.sqrt(calculate_portfolio_var(w,V))\n",
    "    # Marginal Risk Contribution\n",
    "    MRC = V*w.T\n",
    "    # Risk Contribution\n",
    "    RC = np.multiply(MRC,w.T)/sigma\n",
    "    return RC\n",
    "\n",
    "def risk_budget_objective(x,pars):\n",
    "    # calculate portfolio risk\n",
    "    V = pars[0]# covariance table\n",
    "    x_t = pars[1] # risk target in percent of portfolio risk\n",
    "    sig_p =  np.sqrt(calculate_portfolio_var(x,V)) # portfolio sigma\n",
    "    risk_target = np.asmatrix(np.multiply(sig_p,x_t))\n",
    "    asset_RC = calculate_risk_contribution(x,V)\n",
    "    J = sum(np.square(asset_RC-risk_target.T))[0,0] *10000000 # sum of squared error\n",
    "    return J\n",
    "\n",
    "def total_weight_constraint(x):\n",
    "    return np.sum(x)-1.0\n",
    "\n",
    "def long_only_constraint(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import old portfolios to construct staggerd portfolio\n",
    "\n",
    "- Q: How to handel \"hold\" positions?\n",
    "- \"Hold\" companies shold have \"Min Position\" == ACTION/3 rest is weighted from this? and MAX = Average posiotn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 332/482 [01:55<00:47,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "- SDIP.ST: No data found, symbol may be delisted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [02:54<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "### Settings\n",
    "START_DATE = '2016-01-01'\n",
    "error_count = 0\n",
    "error_list = []\n",
    "\n",
    "\n",
    "latest_file= max(glob.glob(\"../equity_data/*.*\"), key=os.path.getmtime)\n",
    "\n",
    "signal_df = pd.read_excel(latest_file, sheet_name=\"Export\")\n",
    "signal_df = signal_df.rename({'Performance - Perform. 3m' : 'Return 3m','Performance - Perform. 6m' : 'Return 6m',\n",
    "                            'Total Return - Return 1y' : 'Return 1y',\n",
    "                            'Div. Yield - Current': 'Yield',\n",
    "                            'Total Equity  - Millions':'Total Equity', 'FCF - Millions': 'FCF','ROE - Current':'ROE',\n",
    "                            'Volatility - St.Dev. 100d':'Volatility','Market Cap - Current': 'Market Cap', \n",
    "                            'ROC - Current':'ROC', 'Tot. Assets - Millions':'Tot. Assets', \n",
    "                            'Gross profit - Millions':'Gross profit', 'Assets Turn - Current': 'Assets Turn',\n",
    "                            'P/FCF - Current':'P/FCF', 'P/E - Current':'P/E', 'P/S - Current':'P/S',\n",
    "                            'P/B - Current':'P/B','EV/EBIT - Current':'EV/EBIT',\n",
    "                            'Info - Country' : 'Country','F-Score - Point':'F-Score',\n",
    "                            'Info - List' : 'List', 'Info - Sector' : 'Sector', 'Info - Industry' : 'Industry',\n",
    "                            'Info - Ticker' : 'Ticker', 'Info - Yahoo':'Yahoo', 'Info - Last Report': 'Last Report',\n",
    "                           'Volume - Average 50d Mill' : 'Volume', 'Tot. Assets - Growth 1y' : 'Asset Growth'}, axis=1)\n",
    "\n",
    "\n",
    "signal_df = signal_df.loc[ (signal_df['List'] != 'Spotlight') \n",
    "                        & (signal_df['List'] != 'NGM') & (signal_df['Country'] == \"Sweden\") &\n",
    "                         (signal_df['Market Cap'] > 200)]\n",
    "\n",
    "signal_df = signal_df.loc[(signal_df['Sector'] != 'Financials')]\n",
    "\n",
    "# Set to dattime\n",
    "signal_df['Last Report'] = pd.to_datetime(signal_df['Last Report'])\n",
    "#set new index\n",
    "signal_df.index = range(len(signal_df.index))\n",
    "\n",
    "\n",
    "signal_df['Res_Mom_1M'] = np.nan\n",
    "signal_df['Res_Mom_1M_alt'] = np.nan\n",
    "\n",
    "signal_df['Tot_Mom_1M'] = np.nan\n",
    "signal_df['Sea_month_5yr'] = np.nan\n",
    "signal_df['idio_vol_20day'] = np.nan\n",
    "signal_df['maxret_5days'] = np.nan\n",
    "signal_df[\"EAR_std\"]= np.nan\n",
    "signal_df[\"5yr_vol\"]= np.nan\n",
    "signal_df[\"liq_shock\"]= np.nan\n",
    "\n",
    "index = yf.download('^OMXSPI',start=START_DATE, threads = False, progress = False)\n",
    "index = index['Adj Close']\n",
    "for i in tqdm(range(len(signal_df))):\n",
    "\n",
    "    try:\n",
    "        stock_tmp = yf.download(signal_df.iloc[i]['Yahoo'],start=START_DATE, progress = False, threads = False)\n",
    "\n",
    "        \n",
    "        stock = stock_tmp['Adj Close']\n",
    "        import_data = pd.concat([stock, index], axis = 1)\n",
    "        import_data.columns = ['stock', 'index']\n",
    "        import_data = import_data.dropna()\n",
    "        \n",
    "        long_df = import_data.copy()\n",
    "        ret_df = np.log(import_data/import_data.shift()).dropna()\n",
    "        ### SEASONALITY\n",
    "        \n",
    "        monthly_df = import_data.resample('M').last()\n",
    "        monthly_ret_df = np.log(monthly_df/monthly_df.shift()).dropna()\n",
    "         \n",
    "        ### 1 Month Momentum\n",
    "        signal_df.loc[i,\"Tot_Mom_1M\"] = ret_df['stock'].tail(21).sum()\n",
    "        \n",
    "        \n",
    "        ##EAR\n",
    "        idx = ret_df.index.get_loc(signal_df.iloc[i]['Last Report'], method='nearest')\n",
    "        \n",
    "        EA_data = import_data.iloc[idx - 2 : idx +2 ]\n",
    "        \n",
    "        EA_ret = (EA_data.pct_change().dropna()+1).cumprod().tail(1)\n",
    "        pead_ret = float(EA_ret['stock'] - EA_ret['index']) #Should use np.log()\n",
    "        pead_vol = np.log(stock.iloc[:idx]/stock.iloc[:idx].shift()).tail(60).std()*252**.5\n",
    "        signal_df.loc[i, 'EAR_std'] = pead_ret/pead_vol\n",
    "        \n",
    "        ## liquidity shock\n",
    "        \n",
    "        stock_volume = stock_tmp.copy()\n",
    "        stock_volume['volume_sek'] = stock_volume['Close'] *stock_volume['Volume']\n",
    "        \n",
    "        # Resample to monthly for sobustness???\n",
    "        stock_volume = stock_volume.rolling(21).sum().resample('30D').last()\n",
    "        \n",
    "\n",
    "        liq_shock = (stock_volume['volume_sek'].tail(1) - stock_volume['volume_sek'].tail(12).mean())/stock_volume['volume_sek'].tail(12).std()\n",
    "        signal_df.loc[i,\"liq_shock\"] = float(liq_shock)\n",
    "        \n",
    "        ### RESIDUAL MOMENTUM\n",
    "        ret_trim_df = ret_df.drop(ret_df.iloc[[idx - 1, idx, idx +1, idx +2 ]].index)\n",
    "        \n",
    "        \n",
    "        signal_df.loc[i,\"maxret_5days\"] = np.mean(sorted(ret_trim_df['stock'].tail(21))[-5:])\n",
    "        y_res = ret_trim_df.tail(21)['stock']\n",
    "        X_res = np.array(ret_trim_df.tail(21)['index']).reshape(-1, 1)\n",
    "        reg_res = LinearRegression().fit(X_res, y_res)\n",
    "        residuals_res = y_res - reg_res.predict(X_res)\n",
    "        signal_df.loc[i,\"idio_vol_20day\"] = residuals_res.std()\n",
    "        \n",
    "        #volume weight for short term rev\n",
    "        vol_weight_tmp = stock_tmp.copy()\n",
    "        vol_weight_tmp['volume_sek'] = vol_weight_tmp['Close'] *vol_weight_tmp['Volume']\n",
    "        #weight by 60 day MA\n",
    "        vol_weight = vol_weight_tmp['volume_sek'].rolling(252).mean() / vol_weight_tmp['volume_sek']\n",
    "        \n",
    "        reg_df = ret_trim_df.tail(3*252)\n",
    "        ## Identify Report \n",
    "        if len(reg_df)>(2*252):\n",
    "            y = reg_df['stock']\n",
    "            X = np.array(reg_df['index']).reshape(-1, 1)\n",
    "            reg = LinearRegression().fit(X, y)\n",
    "            beta = reg.coef_[0]\n",
    "            residuals = y - reg.predict(X)\n",
    "            std_residuals = residuals/residuals.std()\n",
    "            \n",
    "            signal_df.loc[i,\"Res_Mom_1M\"] = std_residuals.tail(21).sum()# + np.log(vol_weight.tail(21)).sum()\n",
    "           \n",
    "        \n",
    "        if len(monthly_ret_df)>=36:\n",
    "            seas_list = []\n",
    "            monthly_vol = 0\n",
    "            for look in [12,24,36,48,60]:\n",
    "                try:\n",
    "                    seas_list.append(monthly_ret_df['stock'].iloc[-look])\n",
    "                    monthly_vol = (monthly_ret_df['stock'].tail(60)+1).std() * np.sqrt(12)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "            signal_df.loc[i,\"Sea_month_5yr\"] = np.mean(seas_list) *12\n",
    "            signal_df.loc[i,\"5yr_vol\"] = monthly_vol\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        error_count = error_count + 1\n",
    "        error_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../equity_data/Borsdata_2022-08-27.xlsx'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### CREATE NEW SECTORS!\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Leisure', 'Gambling & Casinos','Airlines','Hotels']),'Sector'] = 'Travel & Leisure'\n",
    "\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Pharmaceuticals']),'Sector'] = 'Pharmaceuticals'\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Medical Equipment']),'Sector'] = 'Medical Equipment'\n",
    "\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Retailers','Auto & Equipment','Industrial Components', 'Clothing & Footwear',\n",
    "    'Consumer Electronics', 'Accessories' ]),'Sector'] = 'Retail'\n",
    "\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['IT Consulting', 'IT Services', 'Communications',]),'Sector'] = 'Software'\n",
    "\n",
    "\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Industrial Components',\n",
    "    'Energy & Recycling' ]),'Sector'] = 'General Industrials'\n",
    "\n",
    "\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Construction Supplies','Construction & Infrastructure',\n",
    "     'Installation']),'Sector'] = 'Construction & Materials' \n",
    "\n",
    "\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Industrial Machinery', 'Electrical Components']),'Sector'] = 'Electronic & Electrical Equipment'\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Holding Companies']),'Sector'] = 'Holding Companies'\n",
    "\n",
    "\n",
    "signal_df.loc[signal_df['Industry'].isin(\n",
    "    ['Real Estate']),'Sector'] = 'Real Estate'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUTOFF = 0.33 #0.25 # which cut off??\n",
    "\n",
    "method = 'median'\n",
    "\n",
    "### SHORT TERM REVERSAL - adjust for industry ###\n",
    "signal_df['Res_Mom_1M_adj'] = signal_df[\"Res_Mom_1M\"] - signal_df.groupby(\"Sector\")[\"Res_Mom_1M\"].transform(method)\n",
    "\n",
    "\n",
    "#Vol adjsuted Seasonality\n",
    "signal_df['Sea_month_5yr_std'] = signal_df['Sea_month_5yr']/signal_df['5yr_vol']\n",
    "\n",
    "### IVOL - adjust for industry\n",
    "signal_df['idio_vol_20day_adj'] = signal_df[\"idio_vol_20day\"] - signal_df.groupby(\"Sector\")[\"idio_vol_20day\"].transform(method)\n",
    "\n",
    "## MAX RET - adjust for industry\n",
    "signal_df['maxret_5days_adj'] = signal_df[\"maxret_5days\"] - signal_df.groupby(\"Sector\")[\"maxret_5days\"].transform(method)\n",
    "\n",
    "########## INDUSTRY MOMENTUM ASSNESS SHOWS THAT EQUAL WEIGHT WORKS\n",
    "signal_df['Sector Weighted Mom'] = signal_df.groupby(\"Sector\")[\"Tot_Mom_1M\"].transform(method)\n",
    "\n",
    "# IMPUTE MEDIAN VALUE FOR NANS\n",
    "signal_df['Res_Mom_1M_adj'] = signal_df['Res_Mom_1M_adj'].fillna(signal_df['Res_Mom_1M'].median())\n",
    "\n",
    "### SEASONALITY\n",
    "signal_df['Sea_month_5yr_std'] = signal_df['Sea_month_5yr_std'].fillna(signal_df['Sea_month_5yr_std'].median())\n",
    "### SHORT TERM IVOL\n",
    "signal_df['idio_vol_20day_adj'] = signal_df['idio_vol_20day_adj'].fillna(signal_df['idio_vol_20day'].median())\n",
    "### MAX RET\n",
    "signal_df['maxret_5days_adj'] = signal_df['maxret_5days_adj'].fillna(signal_df['maxret_5days_adj'].median())\n",
    "# Ear \n",
    "signal_df['EAR_std'] = signal_df['EAR_std'].fillna(signal_df['EAR_std'].median())\n",
    "# Liquidity Shock\n",
    "signal_df['liq_shock'] = signal_df['liq_shock'].fillna(signal_df['liq_shock'].median())\n",
    "\n",
    "\n",
    "\n",
    "######### RANK ON INDIVIDUAL PREDICTORS\n",
    "signal_df['Res_Mom_1M_adj_rank'] = signal_df['Res_Mom_1M_adj'].rank(ascending=True, pct = True)\n",
    "#high is good\n",
    "signal_df['Sector Momentum Rank'] =  signal_df['Sector Weighted Mom'].rank(ascending=False, pct = True)\n",
    "#high is good\n",
    "signal_df['Seasonality Rank'] =  signal_df['Sea_month_5yr'].rank(ascending=False, pct = True)\n",
    "#high is good\n",
    "signal_df['Seasonality Rank Std'] =  signal_df['Sea_month_5yr_std'].rank(ascending=False, pct = True)\n",
    "#low is good\n",
    "signal_df['IVOL_adj Rank'] =  signal_df['idio_vol_20day_adj'].rank(ascending=True, pct = True)\n",
    "#low is good\n",
    "signal_df['MAXRET_adj Rank'] =  signal_df['maxret_5days_adj'].rank(ascending=True, pct = True)\n",
    "#high is good\n",
    "signal_df['EAR_std Rank'] =  signal_df['EAR_std'].rank(ascending=False, pct = True)\n",
    "#high is good\n",
    "signal_df['liq_shock Rank'] =  signal_df['liq_shock'].rank(ascending=False, pct = True)\n",
    "\n",
    "################# YOU WANT THE LOWEST SCORE POSSIBLE\n",
    "## USE PCT. \n",
    "## Implement an interaction score for liquidity and SREV \n",
    "\n",
    "signal_df['High_Freq_Combo'] = ( signal_df['Sector Momentum Rank'] +\n",
    "                               signal_df['Res_Mom_1M_adj_rank'] +\n",
    "                               signal_df['Seasonality Rank Std'] +  signal_df['EAR_std Rank'] +\n",
    "                                signal_df['liq_shock Rank']+\n",
    "                                0.5*signal_df['IVOL_adj Rank'] + 0.5*signal_df['MAXRET_adj Rank']\n",
    "                                ).rank(ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "signal_df['Signal'] = \"Neutral\"\n",
    "idx_BUY = signal_df['High_Freq_Combo']<=signal_df['High_Freq_Combo'].quantile(CUTOFF)\n",
    "signal_df.loc[idx_BUY,'Signal']= 'Buy'\n",
    "\n",
    "\n",
    "idx_SELL = signal_df['High_Freq_Combo']>=signal_df['High_Freq_Combo'].quantile(1-CUTOFF)\n",
    "signal_df.loc[idx_SELL,'Signal'] = 'Sell'\n",
    "rank_data = signal_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMIZE PORTFOLIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hvol_yz(df, lookback=10):\n",
    "    o = df.Open\n",
    "    h = df.High\n",
    "    l = df.Low\n",
    "    c = df.Close # should this be \n",
    "    k = 0.34 / (1.34 + (lookback+1)/(lookback-1))\n",
    "    cc = np.log(c/c.shift(1))\n",
    "    ho = np.log(h/o)\n",
    "    lo = np.log(l/o)\n",
    "    co = np.log(c/o)\n",
    "    oc = np.log(o/c.shift(1))\n",
    "    oc_sq = oc**2\n",
    "    cc_sq = cc**2\n",
    "    rs = ho*(ho-co)+lo*(lo-co)\n",
    "    #close_vol = pd.rolling_sum(cc_sq, window=lookback) * (1.0 / (lookback - 1.0))\n",
    "    close_vol =  cc_sq.rolling(lookback).sum() * (1.0 / (lookback - 1.0))\n",
    "    open_vol =  oc_sq.rolling(lookback).sum()  * (1.0 / (lookback - 1.0))\n",
    "    window_rs =  rs.rolling(lookback).sum()  * (1.0 / (lookback - 1.0))\n",
    "    result = (open_vol + k * close_vol + (1-k) * window_rs).apply(np.sqrt) \n",
    "    result[:lookback-1] = np.nan\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### IMPORt DATA AND CREATE MA of SIGNAL\n",
    "\n",
    "SPREAD = 3\n",
    "N = 15\n",
    "\n",
    "\n",
    "folder = \"../clean_equity_data/\"\n",
    "\n",
    "port_file = (\"../portfolios/eriks_port.xlsx\")\n",
    "\n",
    "\n",
    "current_port_tmp = pd.read_excel(port_file)\n",
    "current_cash = current_port_tmp.loc[current_port_tmp['Company'].isin([\"Cash\"])]\n",
    "current_port_tmp = current_port_tmp.loc[current_port_tmp['Current %']>0,]\n",
    "current_port_tmp = current_port_tmp.loc[~current_port_tmp['Company'].isin([\"Cash\", \"Total\"])]\n",
    "current_port = current_port_tmp[['Company','Current %' ]]\n",
    "current_port = current_port[~current_port['Company'].isna()]\n",
    "\n",
    "file_list = [\"GESTALT_2022-08-27.csv\",\"GESTALT_2022-08-27.csv\", \"GESTALT_2022-08-27.csv\" ]\n",
    "#file_list = [\"GESTALT_2022-06-29.csv\",\"GESTALT_2022-07-28.csv\", \"GESTALT_2022-08-27.csv\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for idx, file in enumerate(file_list):\n",
    "    data_tmp = pd.read_csv(folder + file)\n",
    "    data_tmp.loc[:, 'Signal_' + str(idx)] =  np.linspace(1, -1, num=len(data_tmp))\n",
    "    data_tmp_clean = data_tmp[['Company','Yahoo', 'Signal_' + str(idx)]]\n",
    "    if idx == 0:\n",
    "        data_comb = data_tmp_clean\n",
    "    else:\n",
    "        data_comb = data_comb.merge(data_tmp_clean, how ='outer', on = ['Company','Yahoo'] )\n",
    "    \n",
    "    \n",
    "#Fill missing values with 0\n",
    "data_comb[data_comb.filter(like='Signal').columns] = data_comb[data_comb.filter(like='Signal').columns].fillna(value=0)\n",
    "\n",
    "### Get avergae signal value over the columns\n",
    "data_comb['Signal_avg'] = data_comb[data_comb.filter(like='Signal').columns].mean(axis = 1)\n",
    "sig_scaled = (2*data_comb['Signal_avg'])/ abs(data_comb['Signal_avg']).sum() #scale as aqr\n",
    "data_comb['Signal_avg'] = sig_scaled\n",
    "\n",
    "data_comb = data_comb.sort_values(by = 'Signal_avg', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### TOP STOCKS, GET \n",
    "\n",
    "top = data_comb[0:SPREAD*N][['Company','Yahoo', 'Signal_avg']] \n",
    "top.loc[:,'assets_risk_budget'] = np.linspace(1, 0.5, num=SPREAD*N)\n",
    "\n",
    "##### SELECT THE PORTFOLIO BY BUY/HOLD SPREAD\n",
    "\n",
    "buy = top[0:N][['Company','Yahoo' ,'Signal_avg', 'assets_risk_budget']]\n",
    "hold = top[N: round(SPREAD*N)][['Company','Yahoo', 'Signal_avg', 'assets_risk_budget']] # update to latest month spread??\n",
    "keep = hold[hold['Company'].isin(current_port['Company'])]\n",
    "    \n",
    "input_opti = pd.concat([buy,keep])\n",
    "\n",
    "## RESCALE ASSETS_RISK_BUDGET\n",
    "\n",
    "rescaled_risk_budget = input_opti['assets_risk_budget']/input_opti['assets_risk_budget'].sum()\n",
    "input_opti['adj_risk_budget'] = rescaled_risk_budget\n",
    "\n",
    "SELL_LIST =pd.DataFrame(list(set(current_port['Company']) - set(input_opti['Company'])),columns = ['Company'] )\n",
    "#SELL_LIST = SELL_LIST.groupby(['Company']).sum()\n",
    "SELL_LIST = SELL_LIST.merge(rank_data[['Company','Signal', \"Ticker\", \"Yahoo\"]], on = 'Company')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT\n",
    "https://thequantmba.wordpress.com/2016/12/14/risk-parityrisk-budgeting-portfolio-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def constr_f(x):\n",
    "    return np.array(sum(x))\n",
    "\n",
    "\n",
    "def ERC_gestalt_alt(data, lookback = 252):\n",
    "    \n",
    "    prices_df = pd.DataFrame()\n",
    "    vol_yz = pd.DataFrame()\n",
    "    for tick in data['Yahoo']:\n",
    "    \n",
    "        price_imp_tmp = yf.download(tick,start='2000-01-01', progress = False, threads = False)\n",
    "        price = price_imp_tmp['Adj Close']\n",
    "        price_tmp = pd.DataFrame(price)\n",
    "        price_tmp.columns = [tick]\n",
    "        prices_df = pd.concat([prices_df, price_tmp], axis = 1)\n",
    "        #append to array instead\n",
    "        vol_yz[tick] = get_hvol_yz(price_imp_tmp,lookback = lookback).tail(1)\n",
    "\n",
    "\n",
    "\n",
    "    vol_yz_ls = vol_yz.values\n",
    "    vol_yz_ls[np.isnan(vol_yz_ls)] = np.nanmean(vol_yz_ls)\n",
    "\n",
    "\n",
    "    log_ret = np.log(prices_df) - np.log(prices_df.shift(1))\n",
    "    log_ret = log_ret.tail(300)\n",
    "    log_ret = log_ret.fillna(log_ret.mean())\n",
    "\n",
    "\n",
    "    #cov = pyRMT.optimalShrinkage(log_ret.tail(252) , return_covariance=True) #we want the regurlized\n",
    "    corr = optimalShrinkage(log_ret.tail(lookback), method = 'IW') #regurlized\n",
    "    cov = vol_yz_ls.T * corr * vol_yz_ls   \n",
    "        \n",
    "        \n",
    "    log_ret = np.log(prices_df) - np.log(prices_df.shift(1))\n",
    "    log_ret = log_ret.dropna()\n",
    "    \n",
    "    \n",
    "     \n",
    "    bounds = [(0.01,0.3)] *len(input_opti)\n",
    "\n",
    "\n",
    "    # sum of weights beteen 0.99 and 1.01\n",
    "    nlc = optimize.NonlinearConstraint(constr_f, 0.99, 1.01)\n",
    "    \n",
    "    x_t =data['adj_risk_budget']# your risk budget percent of total portfolio risk (equal risk)\n",
    "\n",
    "\n",
    "    \n",
    "    # Add constrinarts?\n",
    "    res_diff = optimize.differential_evolution(risk_budget_objective, args=[[cov,x_t]],\n",
    "                                bounds = bounds, maxiter = 10000,\n",
    "                                 constraints = nlc          )\n",
    "    print(res_diff)\n",
    "    weight = res_diff['x']/res_diff['x'].sum()\n",
    "\n",
    "    return(weight, cov)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize\n",
    "res_w, cov = ERC_gestalt_alt(input_opti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_input_opti = input_opti.copy()\n",
    "new_input_opti['new_weight'] = np.array(round_to_multiple(pd.DataFrame(res_w), 0.001))\n",
    "new_input_opti['vol'] = np.sqrt(np.diagonal(cov)) * np.sqrt(252)\n",
    "new_input_opti['risk_budget'] = input_opti['adj_risk_budget']\n",
    "new_input_opti['risk_contrib'] = calculate_risk_contribution(res_w,cov)/calculate_risk_contribution(res_w,cov).sum()\n",
    "\n",
    "new_port = new_input_opti[['Company','Yahoo','Signal_avg','new_weight', 'adj_risk_budget', 'risk_contrib']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_port = new_port.merge(rank_data[['Company','Signal', \"Ticker\"]], on = 'Company')\n",
    "\n",
    "new_port = pd.concat([final_port,SELL_LIST]).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Yahoo</th>\n",
       "      <th>Signal_avg</th>\n",
       "      <th>new_weight</th>\n",
       "      <th>adj_risk_budget</th>\n",
       "      <th>risk_contrib</th>\n",
       "      <th>Signal</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arctic Paper</td>\n",
       "      <td>ARP.ST</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.056884</td>\n",
       "      <td>0.056884</td>\n",
       "      <td>Buy</td>\n",
       "      <td>ARP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dedicare</td>\n",
       "      <td>DEDI.ST</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.056238</td>\n",
       "      <td>0.056238</td>\n",
       "      <td>Buy</td>\n",
       "      <td>DEDI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rottneros</td>\n",
       "      <td>RROS.ST</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.055591</td>\n",
       "      <td>0.055591</td>\n",
       "      <td>Buy</td>\n",
       "      <td>RROS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>International Petroleum</td>\n",
       "      <td>IPCO.ST</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>Buy</td>\n",
       "      <td>IPCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSAB B</td>\n",
       "      <td>SSAB-B.ST</td>\n",
       "      <td>0.012892</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054299</td>\n",
       "      <td>0.054299</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>SSAB B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EnQuest</td>\n",
       "      <td>ENQ.ST</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.053652</td>\n",
       "      <td>0.053652</td>\n",
       "      <td>Buy</td>\n",
       "      <td>ENQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B3 Consulting</td>\n",
       "      <td>B3.ST</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>B3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New Wave</td>\n",
       "      <td>NEWA-B.ST</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.052359</td>\n",
       "      <td>0.052359</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NEWA B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Betsson</td>\n",
       "      <td>BETS-B.ST</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.051713</td>\n",
       "      <td>0.051713</td>\n",
       "      <td>Buy</td>\n",
       "      <td>BETS B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Prevas</td>\n",
       "      <td>PREV-B.ST</td>\n",
       "      <td>0.012450</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.051067</td>\n",
       "      <td>0.051067</td>\n",
       "      <td>Sell</td>\n",
       "      <td>PREV B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ShaMaran</td>\n",
       "      <td>SNM.ST</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>SNM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nilörngruppen</td>\n",
       "      <td>NIL-B.ST</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.049774</td>\n",
       "      <td>0.049774</td>\n",
       "      <td>Buy</td>\n",
       "      <td>NIL B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tethys Oil</td>\n",
       "      <td>TETY.ST</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.049127</td>\n",
       "      <td>0.049127</td>\n",
       "      <td>Buy</td>\n",
       "      <td>TETY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BE Group</td>\n",
       "      <td>BEGR.ST</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.048481</td>\n",
       "      <td>0.048481</td>\n",
       "      <td>Buy</td>\n",
       "      <td>BEGR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lundin Gold</td>\n",
       "      <td>LUG.ST</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.047835</td>\n",
       "      <td>0.047835</td>\n",
       "      <td>Buy</td>\n",
       "      <td>LUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TietoEVRY</td>\n",
       "      <td>TIETOS.ST</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.047188</td>\n",
       "      <td>0.047188</td>\n",
       "      <td>Buy</td>\n",
       "      <td>TIETOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Clas Ohlson</td>\n",
       "      <td>CLAS-B.ST</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.045249</td>\n",
       "      <td>0.045249</td>\n",
       "      <td>Buy</td>\n",
       "      <td>CLAS B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Softronic</td>\n",
       "      <td>SOF-B.ST</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.043310</td>\n",
       "      <td>0.043310</td>\n",
       "      <td>Sell</td>\n",
       "      <td>SOF B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Africa Oil</td>\n",
       "      <td>AOI.ST</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>Buy</td>\n",
       "      <td>AOI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bilia</td>\n",
       "      <td>BILI-A.ST</td>\n",
       "      <td>0.010773</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.038785</td>\n",
       "      <td>0.038785</td>\n",
       "      <td>Sell</td>\n",
       "      <td>BILI A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Company      Yahoo  Signal_avg  new_weight  \\\n",
       "0              Arctic Paper     ARP.ST    0.013245       0.046   \n",
       "1                  Dedicare    DEDI.ST    0.013157       0.055   \n",
       "2                 Rottneros    RROS.ST    0.013068       0.058   \n",
       "3   International Petroleum    IPCO.ST    0.012980       0.042   \n",
       "4                    SSAB B  SSAB-B.ST    0.012892       0.055   \n",
       "5                   EnQuest     ENQ.ST    0.012804       0.039   \n",
       "6             B3 Consulting      B3.ST    0.012715       0.043   \n",
       "7                  New Wave  NEWA-B.ST    0.012627       0.045   \n",
       "8                   Betsson  BETS-B.ST    0.012539       0.061   \n",
       "9                    Prevas  PREV-B.ST    0.012450       0.047   \n",
       "10                 ShaMaran     SNM.ST    0.012362       0.034   \n",
       "11            Nilörngruppen   NIL-B.ST    0.012274       0.049   \n",
       "12               Tethys Oil    TETY.ST    0.012185       0.040   \n",
       "13                 BE Group    BEGR.ST    0.012097       0.040   \n",
       "14              Lundin Gold     LUG.ST    0.012009       0.060   \n",
       "15                TietoEVRY  TIETOS.ST    0.011921       0.084   \n",
       "16              Clas Ohlson  CLAS-B.ST    0.011656       0.059   \n",
       "17                Softronic   SOF-B.ST    0.011391       0.049   \n",
       "18               Africa Oil     AOI.ST    0.010949       0.037   \n",
       "19                    Bilia  BILI-A.ST    0.010773       0.057   \n",
       "\n",
       "    adj_risk_budget  risk_contrib   Signal  Ticker  \n",
       "0          0.056884      0.056884      Buy     ARP  \n",
       "1          0.056238      0.056238      Buy    DEDI  \n",
       "2          0.055591      0.055591      Buy    RROS  \n",
       "3          0.054945      0.054945      Buy    IPCO  \n",
       "4          0.054299      0.054299  Neutral  SSAB B  \n",
       "5          0.053652      0.053652      Buy     ENQ  \n",
       "6          0.053006      0.053006  Neutral      B3  \n",
       "7          0.052359      0.052359  Neutral  NEWA B  \n",
       "8          0.051713      0.051713      Buy  BETS B  \n",
       "9          0.051067      0.051067     Sell  PREV B  \n",
       "10         0.050420      0.050420  Neutral     SNM  \n",
       "11         0.049774      0.049774      Buy   NIL B  \n",
       "12         0.049127      0.049127      Buy    TETY  \n",
       "13         0.048481      0.048481      Buy    BEGR  \n",
       "14         0.047835      0.047835      Buy     LUG  \n",
       "15         0.047188      0.047188      Buy  TIETOS  \n",
       "16         0.045249      0.045249      Buy  CLAS B  \n",
       "17         0.043310      0.043310     Sell   SOF B  \n",
       "18         0.040078      0.040078      Buy     AOI  \n",
       "19         0.038785      0.038785     Sell  BILI A  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = pd.DataFrame()\n",
    "\n",
    "for tick in new_port['Yahoo']:\n",
    "    if tick == 'Cash':\n",
    "        prices_df[tick] = 10\n",
    "    else:\n",
    "        price = yf.download(tick,start='2000-01-01', progress = False, threads = False)\n",
    "        price = price['Adj Close']\n",
    "        prices_df[tick] = price\n",
    "    \n",
    "log_ret = np.log(prices_df) - np.log(prices_df.shift(1))\n",
    "log_ret = log_ret.dropna() \n",
    "\n",
    "hist_port = log_ret.mul(new_port['new_weight'].values, axis=1).sum(axis = 1).tail(252)\n",
    "\n",
    "prices_df = pd.DataFrame()\n",
    "for tick in [\"^OMX\"]:\n",
    "    \n",
    "    price = yf.download(tick,start='2000-01-01', progress = False, threads = False)\n",
    "    price = price['Adj Close']\n",
    "    prices_df[tick] = price\n",
    "log_ret_tmp = np.log(prices_df) - np.log(prices_df.shift(1))\n",
    "log_ret_tmp = log_ret_tmp.dropna()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "y = hist_port\n",
    "X = np.array(log_ret_tmp.tail(252))#.reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_signal = pd.DataFrame([dparser.parse(latest_file,fuzzy=True).strftime(\"%d/%m/%Y\")])\n",
    "date_signal.columns = [\"Date\"]\n",
    "\n",
    "beta = pd.DataFrame([reg.coef_])\n",
    "beta.columns = [\"Beta\"]\n",
    "vol = pd.DataFrame([hist_port.tail(60).std() * np.sqrt(252)])\n",
    "vol.columns = [\"H-Vol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_positions = SPREAD * N\n",
    "\n",
    "clean_write = pd.DataFrame([])\n",
    "\n",
    "Zeros =  [0] * max_positions\n",
    "Blanks = [\" \"] * max_positions\n",
    "\n",
    "clean_write.loc[:,'Antal'] = Zeros\n",
    "clean_write.loc[:,'Weight'] = Zeros\n",
    "clean_write.loc[:,'Company'] = Blanks\n",
    "clean_write.loc[:,'Ticker'] = Blanks\n",
    "clean_write.loc[:,'Signal'] = Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "new_port = new_port.rename({ 'new_weight': 'Weight'}, axis=1)\n",
    "\n",
    "#add antal\n",
    "new_port = new_port.merge(current_port_tmp[['Company','Antal' ]], on = 'Company', how = 'outer')\n",
    "new_port.loc[new_port['Antal'].isna(),'Antal'] = 0\n",
    "new_port.loc[new_port['Weight'].isna(),'Weight'] = 0\n",
    "\n",
    "#new_port = new_port.merge(signal_df[['Company','Ticker' ]], on = 'Company', how = 'left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove cash and insert seperatly\n",
    "\n",
    "new_cash = new_port.loc[new_port['Company'].isin([\"Cash\"])]\n",
    "new_port = new_port.loc[~new_port['Company'].isin([\"Cash\", \"Total\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Write to excel file\n",
    "#current_port_tmp\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "book = load_workbook(port_file)\n",
    "writer = pd.ExcelWriter(port_file, engine='openpyxl') \n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "\n",
    "clean_write\n",
    "## CLEAN OLD FILE\n",
    "clean_write.to_excel(writer, \"Gestalt\",columns=['Company'], index = False, startcol = 1)\n",
    "clean_write.to_excel(writer, \"Gestalt\",columns=['Ticker'], index = False, startcol = 2)\n",
    "clean_write.to_excel(writer, \"Gestalt\",columns=['Antal'], index = False, startcol = 3)\n",
    "clean_write.to_excel(writer, \"Gestalt\",columns=['Weight'], index = False, startcol = 4)\n",
    "clean_write.to_excel(writer, \"Gestalt\",columns=['Signal'], index = False, startcol = 10)\n",
    "\n",
    "\n",
    "## WRITE NEW PORTFOLIO TO FILE\n",
    "new_port.to_excel(writer, \"Gestalt\",columns=['Company'], index = False, startcol = 1)\n",
    "new_port.to_excel(writer, \"Gestalt\",columns=['Ticker'], index = False, startcol = 2)\n",
    "new_port.to_excel(writer, \"Gestalt\",columns=['Antal'], index = False, startcol = 3)\n",
    "new_port.to_excel(writer, \"Gestalt\",columns=['Weight'], index = False, startcol = 4)\n",
    "new_port.to_excel(writer, \"Gestalt\",columns=['Signal'], index = False, startcol = 10)\n",
    "\n",
    "\n",
    "date_signal.to_excel(writer, \"Gestalt\", index = False, startcol = 9, startrow=max_positions +1)\n",
    "beta.to_excel(writer, \"Gestalt\", index = False, startcol = 10, startrow=max_positions +1)\n",
    "vol.to_excel(writer, \"Gestalt\", index = False, startcol = 11, startrow=max_positions +1)\n",
    "\n",
    "\n",
    "\n",
    "new_cash.to_excel(writer, \"Gestalt\",columns=['Weight'], index = False, header = False, startcol = 4, startrow=max_positions +1)\n",
    "\n",
    "\n",
    "\n",
    "writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
