{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "r\"\"\"Python for Random Matrix Theory. This package implements several \n",
    "cleaning schemes for noisy correlation matrices, including \n",
    "the optimal shrinkage, rotationally-invariant estimator\n",
    "to an underlying correlation matrix (as proposed by Joel Bun, \n",
    "Jean-Philippe Bouchaud, Marc Potters and colleagues).\n",
    "Such cleaned correlation matrix are known to improve factor-decomposition\n",
    "via Principal Component Analysis (PCA) and could be of relevance in a variety \n",
    "of contexts, including computational biology.\n",
    "Cleaning schemes also result in much improved out-of-sample risk\n",
    "of Markowitz optimal portfolios, as established over the years\n",
    "in several papers by Jean-Philippe Bouchaud, Marc Potters and collaborators.\n",
    "Some cleaning schemes can be easily adapted from the various shrinkage\n",
    "estimators implemented in the sklearn.covariance module \n",
    "(see the various publications by O. Ledoit and M. Wolf listed below).\n",
    "In addition, it might make sense to perform an empirical estimate\n",
    "of a correlation matrix robust to outliers before proceeding with\n",
    "the cleaning schemes of the present module. Some of those robust estimates\n",
    "have been implemented in the sklearn.covariance module as well. \n",
    "References\n",
    "----------\n",
    "* \"DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDOM MATRICES\",\n",
    "  V. A. Marcenko and L. A. Pastur\n",
    "  Mathematics of the USSR-Sbornik, Vol. 1 (4), pp 457-483\n",
    "* \"A well-conditioned estimator for large-dimensional covariance matrices\",\n",
    "  O. Ledoit and M. Wolf\n",
    "  Journal of Multivariate Analysis, Vol. 88 (2), pp 365-411\n",
    "* \"Improved estimation of the covariance matrix of stock returns with \"\n",
    "  \"an application to portfolio selection\",\n",
    "  O. Ledoit and M. Wolf\n",
    "  Journal of Empirical Finance, Vol. 10 (5), pp 603-621\n",
    "* \"Financial Applications of Random Matrix Theory: a short review\",\n",
    "  J.-P. Bouchaud and M. Potters\n",
    "  arXiv: 0910.1205 [q-fin.ST]\n",
    "* \"Eigenvectors of some large sample covariance matrix ensembles\",\n",
    "  O. Ledoit and S. Peche\n",
    "  Probability Theory and Related Fields, Vol. 151 (1), pp 233-264\n",
    "* \"NONLINEAR SHRINKAGE ESTIMATION OF LARGE-DIMENSIONAL COVARIANCE MATRICES\",\n",
    "  O. Ledoit and M. Wolf\n",
    "  The Annals of Statistics, Vol. 40 (2), pp 1024-1060 \n",
    "* \"Rotational invariant estimator for general noisy matrices\",\n",
    "  J. Bun, R. Allez, J.-P. Bouchaud and M. Potters\n",
    "  arXiv: 1502.06736 [cond-mat.stat-mech]\n",
    "* \"Cleaning large Correlation Matrices: tools from Random Matrix Theory\",\n",
    "  J. Bun, J.-P. Bouchaud and M. Potters\n",
    "  arXiv: 1610.08104 [cond-mat.stat-mech]\n",
    "* \"Direct Nonlinear Shrinkage Estimation of Large-Dimensional Covariance Matrices (September 2017)\", \n",
    "  O. Ledoit and M. Wolf https://ssrn.com/abstract=3047302 or http://dx.doi.org/10.2139/ssrn.3047302\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division, print_function\n",
    "from builtins import reversed\n",
    "from builtins import map, zip\n",
    "from collections import MutableSequence, Sequence\n",
    "import copy\n",
    "from math import ceil\n",
    "from numbers import Complex, Integral, Real\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "__author__ = 'Gregory Giecold and Lionel Ouaknin'\n",
    "__copyright__ = 'Copyright 2017-2022 Gregory Giecold and contributors'\n",
    "__credit__ = 'Gregory Giecold and Lionel Ouaknin'\n",
    "__status__ = 'beta'\n",
    "__version__ = '0.1.0'\n",
    "\n",
    "\n",
    "__all__ = ['clipped', 'directKernel', 'marcenkoPastur', \n",
    "           'optimalShrinkage', 'poolAdjacentViolators', \n",
    "           'stieltjes']\n",
    "\n",
    "\n",
    "def checkDesignMatrix(X):\n",
    "    \"\"\"\n",
    "       Parameters\n",
    "       ----------\n",
    "       X: a matrix of shape (T, N), where T denotes the number\n",
    "           of samples and N labels the number of features.\n",
    "           If T < N, a warning is issued to the user, and the transpose\n",
    "           of X is considered instead.\n",
    "       Returns:\n",
    "       T: type int\n",
    "       N: type int\n",
    "       transpose_flag: type bool\n",
    "           Specify if the design matrix X should be transposed\n",
    "           in view of having less rows than columns.       \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        assert isinstance(X, (np.ndarray, pd.DataFrame, pd.Series,\n",
    "                              MutableSequence, Sequence))\n",
    "    except AssertionError:\n",
    "        raise\n",
    "        sys.exit(1)\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.atleast_2d(X)\n",
    "\n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        warnings.warn(\"The Marcenko-Pastur distribution pertains to \"\n",
    "                      \"the empirical covariance matrix of a random matrix X \"\n",
    "                      \"of shape (T, N). It is assumed that the number of \"\n",
    "                      \"samples T is assumed higher than the number of \"\n",
    "                      \"features N. The transpose of the matrix X submitted \"\n",
    "                      \"at input will be considered in the cleaning schemes \"\n",
    "                      \"for the corresponding correlation matrix.\", UserWarning)\n",
    "        \n",
    "        T, N = reversed(X.shape)\n",
    "        transpose_flag = True\n",
    "    else:\n",
    "        T, N = X.shape\n",
    "        transpose_flag = False\n",
    "        \n",
    "    return T, N, transpose_flag\n",
    "        \n",
    "        \n",
    "def marcenkoPastur(X):\n",
    "    \"\"\"\n",
    "       Parameter\n",
    "       ---------\n",
    "       X: random matrix of shape (T, N), with T denoting the number\n",
    "           of samples, whereas N refers to the number of features.\n",
    "           It is assumed that the variance of the elements of X\n",
    "           has been normalized to unity.           \n",
    "       Returns\n",
    "       -------\n",
    "       (lambda_min, lambda_max): type tuple\n",
    "           Bounds to the support of the Marcenko-Pastur distribution\n",
    "           associated to random matrix X.\n",
    "       rho: type function\n",
    "           The Marcenko-Pastur density.\n",
    "       Reference\n",
    "       ---------\n",
    "       \"DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDOM MATRICES\",\n",
    "       V. A. Marcenko and L. A. Pastur\n",
    "       Mathematics of the USSR-Sbornik, Vol. 1 (4), pp 457-483\n",
    "    \"\"\"\n",
    "\n",
    "    T, N, _ = checkDesignMatrix(X)\n",
    "    q = N / float(T)\n",
    "\n",
    "    lambda_min = (1 - np.sqrt(q))**2\n",
    "    lambda_max = (1 + np.sqrt(q))**2\n",
    "\n",
    "    def rho(x):\n",
    "        ret = np.sqrt((lambda_max - x) * (x - lambda_min))\n",
    "        ret /= 2 * np.pi * q * x\n",
    "        return ret if lambda_min < x < lambda_max else 0.0\n",
    "\n",
    "    return (lambda_min, lambda_max), rho\n",
    "\n",
    "\n",
    "def clipped(X, alpha=None, return_covariance=False):\n",
    "    \"\"\"Clips the eigenvalues of an empirical correlation matrix E \n",
    "       in order to provide a cleaned estimator E_clipped of the \n",
    "       underlying correlation matrix.\n",
    "       Proceeds by keeping the [N * alpha] top eigenvalues and shrinking\n",
    "       the remaining ones by a trace-preserving constant \n",
    "       (i.e. Tr(E_clipped) = Tr(E)).\n",
    "       Parameters\n",
    "       ----------\n",
    "       X: design matrix, of shape (T, N), where T denotes the number\n",
    "           of samples (think measurements in a time series), while N\n",
    "           stands for the number of features (think of stock tickers).\n",
    "       alpha: type float or derived from numbers.Real (default: None)\n",
    "           Parameter between 0 and 1, inclusive, determining the fraction\n",
    "           to keep of the top eigenvalues of an empirical correlation matrix.\n",
    "           If left unspecified, alpha is chosen so as to keep all the\n",
    "           empirical eigenvalues greater than the upper limit of \n",
    "           the support to the Marcenko-Pastur spectrum. Indeed, such \n",
    "           eigenvalues can be considered as associated with some signal,\n",
    "           whereas the ones falling inside the Marcenko-Pastur range\n",
    "           should be considered as corrupted with noise and indistinguishable\n",
    "           from the spectrum of the correlation of a random matrix.\n",
    "           This ignores finite-size effects that make it possible\n",
    "           for the eigenvalues to exceed the upper and lower edges\n",
    "           defined by the Marcenko-Pastur spectrum (cf. a set of results\n",
    "           revolving around the Tracy-Widom distribution)\n",
    "           \n",
    "       return_covariance: type bool (default: False)\n",
    "           If set to True, compute the standard deviations of each individual\n",
    "           feature across observations, clean the underlying matrix\n",
    "           of pairwise correlations, then re-apply the standard\n",
    "           deviations and return a cleaned variance-covariance matrix.\n",
    "       Returns\n",
    "       -------\n",
    "       E_clipped: type numpy.ndarray, shape (N, N)\n",
    "           Cleaned estimator of the true correlation matrix C underlying\n",
    "           a noisy, in-sample estimate E (empirical correlation matrix\n",
    "           estimated from X). This cleaned estimator proceeds through\n",
    "           a simple eigenvalue clipping procedure (cf. reference below).\n",
    "           \n",
    "           If return_covariance=True, E_clipped corresponds to a cleaned \n",
    "           variance-covariance matrix.\n",
    "       Reference\n",
    "       ---------\n",
    "       \"Financial Applications of Random Matrix Theory: a short review\",\n",
    "       J.-P. Bouchaud and M. Potters\n",
    "       arXiv: 0910.1205 [q-fin.ST]\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if alpha is not None:\n",
    "            assert isinstance(alpha, Real) and 0 <= alpha <= 1\n",
    "            \n",
    "        assert isinstance(return_covariance, bool)\n",
    "    except AssertionError:\n",
    "        raise\n",
    "        sys.exit(1)\n",
    "    \n",
    "    T, N, transpose_flag = checkDesignMatrix(X)\n",
    "    if transpose_flag:\n",
    "        X = X.T\n",
    "        \n",
    "    if not return_covariance:\n",
    "        X = StandardScaler(with_mean=False,\n",
    "                           with_std=True).fit_transform(X)\n",
    "\n",
    "    ec = EmpiricalCovariance(store_precision=False,\n",
    "                             assume_centered=True)\n",
    "    ec.fit(X)\n",
    "    E = ec.covariance_\n",
    "    \n",
    "    if return_covariance:\n",
    "        inverse_std = 1./np.sqrt(np.diag(E))\n",
    "        E *= inverse_std\n",
    "        E *= inverse_std.reshape(-1, 1)\n",
    "\n",
    "    eigvals, eigvecs = np.linalg.eigh(E)\n",
    "    eigvecs = eigvecs.T\n",
    "\n",
    "    if alpha is None:\n",
    "        (lambda_min, lambda_max), _ = marcenkoPastur(X)\n",
    "        xi_clipped = np.where(eigvals >= lambda_max, eigvals, np.nan)\n",
    "    else:\n",
    "        xi_clipped = np.full(N, np.nan)\n",
    "        threshold = int(ceil(alpha * N))\n",
    "        if threshold > 0:\n",
    "            xi_clipped[-threshold:] = eigvals[-threshold:]\n",
    "\n",
    "    gamma = float(E.trace() - np.nansum(xi_clipped))\n",
    "    gamma /= np.isnan(xi_clipped).sum()\n",
    "    xi_clipped = np.where(np.isnan(xi_clipped), gamma, xi_clipped)\n",
    "\n",
    "    E_clipped = np.zeros((N, N), dtype=float)\n",
    "    for xi, eigvec in zip(xi_clipped, eigvecs):\n",
    "        eigvec = eigvec.reshape(-1, 1)\n",
    "        E_clipped += xi * eigvec.dot(eigvec.T)\n",
    "        \n",
    "    tmp = 1./np.sqrt(np.diag(E_clipped))\n",
    "    E_clipped *= tmp\n",
    "    E_clipped *= tmp.reshape(-1, 1)\n",
    "    \n",
    "    if return_covariance:\n",
    "      std = 1./inverse_std\n",
    "      E_clipped *= std\n",
    "      E_clipped *= std.reshape(-1, 1)\n",
    "\n",
    "    return E_clipped\n",
    "\n",
    "\n",
    "def stieltjes(z, E):\n",
    "    \"\"\"\n",
    "       Parameters\n",
    "       ----------\n",
    "       z: complex number\n",
    "       E: square matrix\n",
    "       Returns\n",
    "       -------\n",
    "       A complex number, the resolvent of square matrix E, \n",
    "       also known as its Stieltjes transform.\n",
    "       Reference\n",
    "       ---------\n",
    "       \"Financial Applications of Random Matrix Theory: a short review\",\n",
    "       J.-P. Bouchaud and M. Potters\n",
    "       arXiv: 0910.1205 [q-fin.ST]\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        assert isinstance(z, Complex)\n",
    "        \n",
    "        assert isinstance(E, (np.ndarray, pd.DataFrame,\n",
    "                              MutableSequence, Sequence))\n",
    "        E = np.asarray(E, dtype=float)\n",
    "        E = np.atleast_2d(E)\n",
    "        assert E.shape[0] == E.shape[1]\n",
    "    except AssertionError:\n",
    "        raise\n",
    "        sys.exit(1)\n",
    "\n",
    "    N = E.shape[0]\n",
    "    \n",
    "    ret = z * np.eye(N, dtype=float) - E\n",
    "    ret = np.trace(ret) / N\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def xiHelper(x, q, E):\n",
    "    \"\"\"Helper function to the rotationally-invariant, optimal shrinkage\n",
    "       estimator of the true correlation matrix (implemented via function\n",
    "       optimalShrinkage of the present module). \n",
    "       Parameters\n",
    "       ----------\n",
    "       x: type derived from numbers.Real\n",
    "           Would typically be expected to be an eigenvalue from the\n",
    "           spectrum of correlation matrix E. The present function\n",
    "           can however handle an arbitrary floating-point number.\n",
    "       q: type derived from numbers.Real\n",
    "           The number parametrizing a Marcenko-Pastur spectrum.\n",
    "       E: type numpy.ndarray\n",
    "           Symmetric correlation matrix associated with the \n",
    "           Marcenko-Pastur parameter q specified above.\n",
    "       Returns\n",
    "       -------\n",
    "       xi: type float\n",
    "           Cleaned eigenvalue of the true correlation matrix C underlying\n",
    "           the empirical correlation E (the latter being corrupted \n",
    "           with in-sample noise). This cleaned version is computed\n",
    "           assuming no prior knowledge on the structure of the true\n",
    "           eigenvectors (thereby leaving the eigenvectors of E unscathed). \n",
    "       References\n",
    "       ----------\n",
    "       * \"Rotational invariant estimator for general noisy matrices\",\n",
    "         J. Bun, R. Allez, J.-P. Bouchaud and M. Potters\n",
    "         arXiv: 1502.06736 [cond-mat.stat-mech]\n",
    "       * \"Cleaning large Correlation Matrices: tools from Random Matrix Theory\",\n",
    "         J. Bun, J.-P. Bouchaud and M. Potters\n",
    "         arXiv: 1610.08104 [cond-mat.stat-mech]\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        assert isinstance(x, Real)\n",
    "        assert isinstance(q, Real)\n",
    "        assert isinstance(E, np.ndarray) and E.shape[0] == E.shape[1]\n",
    "        assert np.allclose(E.transpose(1, 0), E)\n",
    "    except AssertionError:\n",
    "        raise\n",
    "        sys.exit(1)\n",
    "\n",
    "    N = E.shape[0]\n",
    "    \n",
    "    z = x - 1j / np.sqrt(N)\n",
    "    s = stieltjes(z, E)\n",
    "    xi = x / abs(1 - q + q * z * s)**2\n",
    "\n",
    "    return xi\n",
    "\n",
    "\n",
    "def gammaHelper(x, q, N, lambda_N, inverse_wishart=False):\n",
    "    \"\"\"Helper function to optimalShrinkage function defined below.\n",
    "       The eigenvalue to the cleaned estimator of a true correlation\n",
    "       matrix are computed via the function xiHelper defined above in\n",
    "       the module at hand. \n",
    "       \n",
    "       It is known however that when N is not very large\n",
    "       a systematic downward bias affects the xiHelper estimator for small\n",
    "       eigenvalues of the noisy empirical correlation matrix. This bias\n",
    "       can be heuristically corrected by computing\n",
    "       xi_hat = xi_RIE * max(1, Gamma),\n",
    "       with Gamma evaluated by the function gammaHelper herewith.\n",
    "       Parameters\n",
    "       ----------\n",
    "       x: type float or any other type derived from numbers.Real\n",
    "           Typically an eigenvalue from the spectrum of a sample\n",
    "           estimate of the correlation matrix associated to some\n",
    "           design matrix X. However, the present function supports\n",
    "           any arbitrary floating-point number x at input.\n",
    "       q: type derived from numbers.Real\n",
    "           Parametrizes a Marcenko-Pastur spectrum.\n",
    "       N: type derived from numbers.Integral\n",
    "           Dimension of a correlation matrix whose debiased, \n",
    "           rotationally-invariant estimator is to be assessed via\n",
    "           the function RIE (see below), of which the present function\n",
    "           is a helper.\n",
    "       lambda_N: type derived from numbers.Real\n",
    "           Smallest eigenvalue from the spectrum of an empirical\n",
    "           estimate to a correlation matrix.\n",
    "        \n",
    "       inverse_wishart: type bool default: False\n",
    "            Wether to use inverse wishart regularization\n",
    "       Returns\n",
    "       ------\n",
    "       Gamma: type float\n",
    "           Upward correction factor for computing a debiased \n",
    "           rotationally-invariant estimator of a true underlying \n",
    "           correlation matrix. \n",
    "       Reference\n",
    "       ---------\n",
    "       \"Cleaning large Correlation Matrices: tools from Random Matrix Theory\",\n",
    "        J. Bun, J.-P. Bouchaud and M. Potters\n",
    "        arXiv: 1610.08104 [cond-mat.stat-mech]\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        assert isinstance(x, Real)\n",
    "        assert isinstance(q, Real)\n",
    "        assert isinstance(N, Integral)\n",
    "        assert isinstance(lambda_N, Real)\n",
    "    except AssertionError:\n",
    "        raise\n",
    "        sys.exit(1)\n",
    "\n",
    "    z = x - 1j / np.sqrt(N)\n",
    "    \n",
    "    lambda_plus = (1 + np.sqrt(q))**2\n",
    "    lambda_plus /= (1 - np.sqrt(q))**2\n",
    "    lambda_plus *= lambda_N\n",
    "    sigma_2 = lambda_N / (1 - np.sqrt(q))**2\n",
    "\n",
    "    # gmp defined below stands for the Stieltjes transform of the\n",
    "    # rescaled Marcenko-Pastur density, evaluated at z\n",
    "    gmp = z + sigma_2 * (q - 1) - np.sqrt((z - lambda_N) * (z - lambda_plus))\n",
    "    gmp /= 2 * q * sigma_2 * z\n",
    "\n",
    "    Gamma = abs(1 - q + q * z * gmp)**2\n",
    "    Gamma *= sigma_2\n",
    "    \n",
    "    if inverse_wishart:\n",
    "        kappa = 2 * lambda_N / ((1 - q - lambda_N) ** 2 - 4 * q * lambda_N)\n",
    "        alpha_s = 1 / (1 + 2 * q * kappa)\n",
    "        denom = x / (1 + alpha_s * (x - 1.))\n",
    "        Gamma /= denom\n",
    "    else: \n",
    "        Gamma /= x\n",
    "    \n",
    "    return Gamma\n",
    "\n",
    "\n",
    "def optimalShrinkage(X, return_covariance=False, method='rie'):\n",
    "    \"\"\"This function computes a cleaned, optimal shrinkage, \n",
    "       rotationally-invariant estimator (RIE) of the true correlation \n",
    "       matrix C underlying the noisy, in-sample estimate \n",
    "       E = 1/T X * transpose(X)\n",
    "       associated to a design matrix X of shape (T, N) (T measurements \n",
    "       and N features).\n",
    "       One approach to getting a cleaned estimator that predates the\n",
    "       optimal shrinkage, RIE estimator consists in inverting the \n",
    "       Marcenko-Pastur equation so as to replace the eigenvalues\n",
    "       from the spectrum of E by an estimation of the true ones.\n",
    "       This approach is known to be numerically-unstable, in addition\n",
    "       to failing to account for the overlap between the sample eigenvectors\n",
    "       and the true eigenvectors. How to compute such overlaps was first\n",
    "       explained by Ledoit and Peche (cf. reference below). Their procedure\n",
    "       was extended by Bun, Bouchaud and Potters, who also correct\n",
    "       for a systematic downward bias in small eigenvalues.\n",
    "       \n",
    "       It is this debiased, optimal shrinkage, rotationally-invariant\n",
    "       estimator that the function at hand implements.\n",
    "       \n",
    "       In addition to above method, this funtion also provides access to:  \n",
    "       - The finite N regularization of the optimal RIE for small eigenvalues\n",
    "         as provided in section 8.1 of [3] a.k.a the inverse wishart (IW) regularization.\n",
    "       - The direct kernel method of O. Ledoit and M. Wolf in their 2017 paper [4]. \n",
    "         This is a direct port of their Matlab code.\n",
    "        \n",
    "         \n",
    "       Parameters\n",
    "       ----------\n",
    "       X: design matrix, of shape (T, N), where T denotes the number\n",
    "           of samples (think measurements in a time series), while N\n",
    "           stands for the number of features (think of stock tickers).\n",
    "           \n",
    "       return_covariance: type bool (default: False)\n",
    "           If set to True, compute the standard deviations of each individual\n",
    "           feature across observations, clean the underlying matrix\n",
    "           of pairwise correlations, then re-apply the standard\n",
    "           deviations and return a cleaned variance-covariance matrix.\n",
    "       \n",
    "       method: type string, optional (default=\"rie\")\n",
    "           - If \"rie\" : optimal shrinkage in the manner of Bun & al.\n",
    "            with no regularisation  \n",
    "           - If \"iw\" : optimal shrinkage in the manner of Bun & al.\n",
    "            with the so called Inverse Wishart regularization\n",
    "           - If 'kernel': Direct kernel method of Ledoit  Wolf.\n",
    "       Returns\n",
    "       -------\n",
    "       E_RIE: type numpy.ndarray, shape (N, N)\n",
    "           Cleaned estimator of the true correlation matrix C. A sample\n",
    "           estimator of C is the empirical covariance matrix E \n",
    "           estimated from X. E is corrupted by in-sample noise.\n",
    "           E_RIE is the optimal shrinkage, rotationally-invariant estimator \n",
    "           (RIE) of C computed following the procedure of Joel Bun \n",
    "           and colleagues (cf. references below).\n",
    "           \n",
    "           If return_covariance=True, E_clipped corresponds to a cleaned\n",
    "           variance-covariance matrix.\n",
    "       References\n",
    "       ----------\n",
    "       1 \"Eigenvectors of some large sample covariance matrix ensembles\",\n",
    "         O. Ledoit and S. Peche\n",
    "         Probability Theory and Related Fields, Vol. 151 (1), pp 233-264\n",
    "       2 \"Rotational invariant estimator for general noisy matrices\",\n",
    "         J. Bun, R. Allez, J.-P. Bouchaud and M. Potters\n",
    "         arXiv: 1502.06736 [cond-mat.stat-mech]\n",
    "       3 \"Cleaning large Correlation Matrices: tools from Random Matrix Theory\",\n",
    "         J. Bun, J.-P. Bouchaud and M. Potters\n",
    "         arXiv: 1610.08104 [cond-mat.stat-mech]\n",
    "       4 \"Direct Nonlinear Shrinkage Estimation of Large-Dimensional Covariance Matrices (September 2017)\", \n",
    "         O. Ledoit and M. Wolf https://ssrn.com/abstract=3047302 or http://dx.doi.org/10.2139/ssrn.3047302\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        assert isinstance(return_covariance, bool)\n",
    "    except AssertionError:\n",
    "        raise\n",
    "        sys.exit(1)\n",
    "\n",
    "    T, N, transpose_flag = checkDesignMatrix(X)\n",
    "    if transpose_flag:\n",
    "        X = X.T\n",
    "        \n",
    "    if not return_covariance:\n",
    "        X = StandardScaler(with_mean=False,\n",
    "                           with_std=True).fit_transform(X)\n",
    "\n",
    "    ec = EmpiricalCovariance(store_precision=False,\n",
    "                             assume_centered=True)\n",
    "    ec.fit(X)\n",
    "    E = ec.covariance_\n",
    "    \n",
    "    if return_covariance:\n",
    "        inverse_std = 1./np.sqrt(np.diag(E))\n",
    "        E *= inverse_std\n",
    "        E *= inverse_std.reshape(-1, 1)\n",
    "\n",
    "    eigvals, eigvecs = np.linalg.eigh(E)\n",
    "    eigvecs = eigvecs.T\n",
    "\n",
    "    q = N / float(T)\n",
    "    lambda_N = eigvals[0]  # The smallest empirical eigenvalue,\n",
    "                           # given that the function used to compute\n",
    "                           # the spectrum of a Hermitian or symmetric\n",
    "                           # matrix - namely np.linalg.eigh - returns\n",
    "                           # the eigenvalues in ascending order.\n",
    "    lambda_hats = None\n",
    "    \n",
    "    if method is not 'kernel':\n",
    "        use_inverse_wishart = (method == 'iw')\n",
    "        xis = map(lambda x: xiHelper(x, q, E), eigvals)\n",
    "        Gammas = map(lambda x: gammaHelper(x, q, N, lambda_N, inverse_wishart=use_inverse_wishart), eigvals)\n",
    "        xi_hats = map(lambda a, b: a * b if b > 1 else a, xis, Gammas)\n",
    "        lambda_hats = xi_hats\n",
    "    else:\n",
    "         lambda_hats = directKernel(q, T, N, eigvals)\n",
    "        \n",
    "    E_RIE = np.zeros((N, N), dtype=float)\n",
    "    for lambda_hat, eigvec in zip(lambda_hats, eigvecs):\n",
    "        eigvec = eigvec.reshape(-1, 1)\n",
    "        E_RIE += lambda_hat * eigvec.dot(eigvec.T)\n",
    "        \n",
    "    tmp = 1./np.sqrt(np.diag(E_RIE))\n",
    "    E_RIE *= tmp\n",
    "    E_RIE *= tmp.reshape(-1, 1)\n",
    "    \n",
    "    if return_covariance:\n",
    "        std = 1./inverse_std\n",
    "        E_RIE *= std\n",
    "        E_RIE *= std.reshape(-1, 1)\n",
    "\n",
    "    return E_RIE\n",
    "\n",
    "  \n",
    "def directKernel(q, T, N, eigvals):\n",
    "    \"\"\"This function computes a non linear shrinkage estimator of a covariance marix\n",
    "       based on the spectral distribution of its eigenvalues and that of its Hilbert Tranform.\n",
    "       This is an extension of Ledoit & Péché(2011).\n",
    "       \n",
    "       This is a port of the Matlab code provided by O. Ledoit and M .Wolf. This port \n",
    "       uses the Pool Adjacent Violators (PAV) algorithm by Alexandre Gramfort \n",
    "       (EMAP toolbox). See below for a Python implementation of PAV.\n",
    "                \n",
    "       Parameters\n",
    "       ----------\n",
    "       q: type derived from numbers.Real\n",
    "           Ratio of N/T\n",
    "           \n",
    "       T: type derived from numbers.Integral\n",
    "          Number of samples\n",
    "    \n",
    "       N: type derived from numbers.Integral\n",
    "           Dimension of a correlation matrix\n",
    "       \n",
    "       eigvals: Vector of the covariance matrix eigenvalues\n",
    "       \n",
    "       Returns\n",
    "       -------\n",
    "       dhats: A vector of eigenvalues estimates\n",
    "       \n",
    "       References\n",
    "       ----------\n",
    "       * \"Eigenvectors of some large sample covariance matrix ensembles\",\n",
    "         O. Ledoit and S. Peche (2011)\n",
    "       * \"Direct Nonlinear Shrinkage Estimation of Large-Dimensional Covariance Matrices (September 2017)\", \n",
    "         O. Ledoit and M. Wolf https://ssrn.com/abstract=3047302 or http://dx.doi.org/10.2139/ssrn.3047302\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute direct kernel estimator\n",
    "    lambdas = eigvals[max(0, N - T):].T  # transpose to have a column vector\n",
    "    \n",
    "    h = np.power(T, -0.35)  # Equation (5.4)\n",
    "    h_squared = h ** 2\n",
    "    \n",
    "    L = np.matlib.repmat(lambdas, N, 1).T\n",
    "    Lt = L.transpose()\n",
    "    square_Lt = h_squared * (Lt ** 2)\n",
    "    \n",
    "    zeros = np.zeros((N, N))\n",
    "    \n",
    "    tmp = np.sqrt(np.maximum(4 * square_Lt - (L - Lt) ** 2, zeros)) / (2 * np.pi * square_Lt)\n",
    "    f_tilde = np.mean(tmp, axis=0)    # Equation (5.2)\n",
    "    \n",
    "    tmp = np.sign(L - Lt) * np.sqrt(np.maximum((L - Lt) ** 2 - 4 * square_Lt, zeros)) - L + Lt \n",
    "    tmp /= 2 * np.pi * square_Lt\n",
    "    Hf_tilde = np.mean(tmp, axis=1)    # Equation (5.3)\n",
    "    \n",
    "    if N <= T:\n",
    "        tmp = (np.pi * q * lambdas * f_tilde) ** 2\n",
    "        tmp += (1 - q - np.pi * q * lambdas * Hf_tilde) ** 2\n",
    "        d_tilde = lambdas / tmp    # Equation (4.3)\n",
    "    else:\n",
    "        Hf_tilde_0 = (1 - np.sqrt(1 - 4 * h_squared)) / (2 * np.pi * h_squared) * np.mean(1. / lambdas)  # Equation (C.8)\n",
    "        d_tilde_0 = 1 / (np.pi * (N - T) / T * Hf_tilde_0)  # Equation (C.5)\n",
    "        d_tilde_1 = lambdas / ((np.pi ** 2) * (lambdas ** 2) * (f_tilde ** 2 + Hf_tilde ** 2))  # Equation (C.4)\n",
    "        d_tilde = np.concatenate(np.dot(d_tilde_0, np.ones(N - T, 1, np.float)), d_tilde_1)\n",
    "        \n",
    "    d_hats = poolAdjacentViolators(d_tilde) # Equation (4.5)\n",
    "    \n",
    "    return d_hats\n",
    "\n",
    "  \n",
    "# Author : Alexandre Gramfort\n",
    "# license : BSD\n",
    "def poolAdjacentViolators(y):\n",
    "    \"\"\"\n",
    "    PAV uses the pool adjacent violators method to produce a monotonic smoothing of y.\n",
    "    Translated from matlab by Sean Collins (2006), and part of the EMAP toolbox.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    try:\n",
    "        assert y.ndim == 1\n",
    "    except AssertionError:\n",
    "      raise\n",
    "      sys.exit(1)\n",
    "      \n",
    "    n_samples = len(y)\n",
    "    v = y.copy()\n",
    "    lvls = np.arange(n_samples)\n",
    "    lvlsets = np.c_[lvls, lvls]\n",
    "    \n",
    "    while True:\n",
    "        deriv = np.diff(v)\n",
    "        if np.all(deriv >= 0):\n",
    "            break\n",
    "\n",
    "        violator = np.where(deriv < 0)[0]\n",
    "        start = lvlsets[violator[0], 0]\n",
    "        last = lvlsets[violator[0] + 1, 1]\n",
    "        s = 0\n",
    "        n = last - start + 1\n",
    "        for i in range(start, last + 1):\n",
    "            s += v[i]\n",
    "\n",
    "        val = s / n\n",
    "        for i in range(start, last + 1):\n",
    "            v[i] = val\n",
    "            lvlsets[i, 0] = start\n",
    "            lvlsets[i, 1] = last\n",
    "            \n",
    "    return v\n",
    "\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
